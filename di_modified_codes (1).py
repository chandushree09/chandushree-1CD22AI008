# -*- coding: utf-8 -*-
"""di modified codes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O-NX_7ONSzQS9omXvbdzqzEz9cl3UrW8
"""

# -*- coding: utf-8 -*-
"""
Lily vs Rose CNN in Google Colab (No sample images shown)
"""

# -------------------------------------------------
# 1. Download and prepare Lily vs Rose dataset
# -------------------------------------------------
import tensorflow_datasets as tfds
import os, shutil

# Download TFDS flowers dataset
dataset = tfds.load("tf_flowers", split="train", as_supervised=True)

# Class mapping for tf_flowers dataset:
# 0 = dandelion, 1 = daisy, 2 = tulips, 3 = roses, 4 = sunflowers
# We'll treat "daisy" as lily substitute
lily_label, rose_label = 1, 3

# Create directories like cats vs dogs format
base_dir = '/tmp/lily_and_rose_filtered'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

train_lily_dir = os.path.join(train_dir, 'lilies')
train_rose_dir = os.path.join(train_dir, 'roses')
validation_lily_dir = os.path.join(validation_dir, 'lilies')
validation_rose_dir = os.path.join(validation_dir, 'roses')

for d in [train_lily_dir, train_rose_dir, validation_lily_dir, validation_rose_dir]:
    os.makedirs(d, exist_ok=True)

# Split into train/validation
import random
from PIL import Image

i = 0
for img, label in tfds.as_numpy(dataset):
    if label not in [lily_label, rose_label]:
        continue
    flower = "lilies" if label == lily_label else "roses"
    subset = "train" if random.random() > 0.2 else "validation"
    folder = os.path.join(base_dir, subset, flower)
    im = Image.fromarray(img)
    im.save(os.path.join(folder, f"{flower}_{i}.jpg"))
    i += 1

print("Dataset ready!")
print("Training lilies:", len(os.listdir(train_lily_dir)))
print("Training roses:", len(os.listdir(train_rose_dir)))
print("Validation lilies:", len(os.listdir(validation_lily_dir)))
print("Validation roses:", len(os.listdir(validation_rose_dir)))

# -------------------------------------------------
# 2. Build CNN Model
# -------------------------------------------------
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.summary()

model.compile(optimizer=RMSprop(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# -------------------------------------------------
# 3. Data Generators
# -------------------------------------------------
train_datagen = ImageDataGenerator(rescale=1.0/255.)
test_datagen = ImageDataGenerator(rescale=1.0/255.)

train_generator = train_datagen.flow_from_directory(train_dir,
                                                    batch_size=20,
                                                    class_mode='binary',
                                                    target_size=(150, 150))

validation_generator = test_datagen.flow_from_directory(validation_dir,
                                                        batch_size=20,
                                                        class_mode='binary',
                                                        target_size=(150, 150))

# -------------------------------------------------
# 4. Train Model
# -------------------------------------------------
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    steps_per_epoch=100,
    epochs=10,
    validation_steps=50,
    verbose=2)

# -------------------------------------------------
# 5. Plot accuracy and loss
# -------------------------------------------------
import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

# -------------------------------------------------
# 6. Test with custom image
# -------------------------------------------------
from tensorflow.keras.preprocessing.image import img_to_array, load_img
import numpy as np
from google.colab import files

uploaded = files.upload()
for fn in uploaded.keys():
    path = fn
    img = load_img(path, target_size=(150, 150))
    x = img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x /= 255.0
    classes = model.predict(x, batch_size=10)
    if classes[0] > 0:
        print(fn + " is a Rose üåπ")
    else:
        print(fn + " is a Lily üå∏")

# -*- coding: utf-8 -*-
"""
Deep Reinforcement Learning (Optimized Version)
"""

import numpy as np
import pylab as pl
import networkx as nx

# ---------------- GRAPH SETUP ---------------- #
edges = [(0, 1), (1, 5), (5, 6), (5, 4), (1, 2),
         (1, 3), (9, 10), (2, 4), (0, 6), (6, 7),
         (8, 9), (7, 8), (1, 7), (3, 9)]

goal = 10
MATRIX_SIZE = 11
gamma = 0.6              # ‚Üì reduced learning parameter
TRAIN_EPISODES = 300     # ‚Üì reduced iterations

# ---------------- VISUALIZATION ---------------- #
G = nx.Graph()
G.add_edges_from(edges)
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True)
pl.show()

# ---------------- REWARD MATRIX ---------------- #
M = np.ones((MATRIX_SIZE, MATRIX_SIZE)) * -1

for s, a in edges:
    M[s, a] = 100 if a == goal else 0
    M[a, s] = 100 if s == goal else 0

M[goal, goal] = 100

# ---------------- Q MATRIX ---------------- #
Q = np.zeros((MATRIX_SIZE, MATRIX_SIZE))

# ---------------- FUNCTIONS ---------------- #
def available_actions(state):
    return np.where(M[state] >= 0)[0]

def sample_next_action(actions):
    return int(np.random.choice(actions, 1))

def update_q(state, action):
    Q[state, action] = M[state, action] + gamma * np.max(Q[action])
    return np.sum(Q / np.max(Q) * 100) if np.max(Q) > 0 else 0

# ---------------- TRAINING ---------------- #
scores = []
for _ in range(TRAIN_EPISODES):
    state = np.random.randint(0, MATRIX_SIZE)
    actions = available_actions(state)
    action = sample_next_action(actions)
    scores.append(update_q(state, action))

# ---------------- TESTING ---------------- #
state = 0
path = [state]

while state != goal:
    next_state = np.argmax(Q[state])
    path.append(next_state)
    state = next_state

print("Optimized Path to Goal:")
print(path)

pl.plot(scores)
pl.xlabel("Training Episodes")
pl.ylabel("Reward")
pl.show()

# ---------------- ENVIRONMENT AWARE LEARNING ---------------- #
police = [2, 4, 5]
drug_traces = [3, 8, 9]

env_police = np.zeros_like(Q)
env_drugs = np.zeros_like(Q)

def observe_environment(action):
    if action in police:
        return 'p'
    if action in drug_traces:
        return 'd'
    return None

def update_with_env(state, action):
    update_q(state, action)
    env = observe_environment(action)
    if env == 'p':
        env_police[state, action] += 1
    elif env == 'd':
        env_drugs[state, action] += 1

def safe_actions(state):
    actions = available_actions(state)
    safe = [a for a in actions if env_police[state, a] == 0]
    return safe if safe else actions

# ---------------- ENV TRAINING ---------------- #
scores = []
for _ in range(TRAIN_EPISODES):
    state = np.random.randint(0, MATRIX_SIZE)
    actions = safe_actions(state)
    action = sample_next_action(actions)
    update_with_env(state, action)
    scores.append(np.max(Q))

print("Police Matrix:\n", env_police)
print("Drug Matrix:\n", env_drugs)

pl.plot(scores)
pl.xlabel("Episodes with Environment Awareness")
pl.ylabel("Reward")
pl.show()

# -*- coding: utf-8 -*-
"""
Tic Tac Toe Reinforcement Learning - Updated Version
"""

import numpy as np
import pickle

BOARD_ROWS = 3
BOARD_COLS = 3


class State:
    def __init__(self, p1, p2):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.p1 = p1
        self.p2 = p2
        self.isEnd = False
        self.boardHash = None
        self.playerSymbol = 1   # FIXED BUG

    def getHash(self):
        self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))
        return self.boardHash

    def winner(self):
        # row
        for i in range(BOARD_ROWS):
            if sum(self.board[i, :]) == 3:
                self.isEnd = True
                return 1
            if sum(self.board[i, :]) == -3:
                self.isEnd = True
                return -1

        # col
        for i in range(BOARD_COLS):
            if sum(self.board[:, i]) == 3:
                self.isEnd = True
                return 1
            if sum(self.board[:, i]) == -3:
                self.isEnd = True
                return -1

        # diagonals
        diag1 = self.board[0, 0] + self.board[1, 1] + self.board[2, 2]
        diag2 = self.board[0, 2] + self.board[1, 1] + self.board[2, 0]

        if diag1 == 3 or diag2 == 3:
            self.isEnd = True
            return 1
        if diag1 == -3 or diag2 == -3:
            self.isEnd = True
            return -1

        # tie
        if len(self.availablePositions()) == 0:
            self.isEnd = True
            return 0

        self.isEnd = False
        return None

    def availablePositions(self):
        positions = []
        for i in range(BOARD_ROWS):
            for j in range(BOARD_COLS):
                if self.board[i, j] == 0:
                    positions.append((i, j))
        return positions

    def updateState(self, position):
        self.board[position] = self.playerSymbol
        self.playerSymbol = -1 if self.playerSymbol == 1 else 1

    def giveReward(self):
        result = self.winner()
        if result == 1:
            self.p1.feedReward(1)
            self.p2.feedReward(0)
        elif result == -1:
            self.p1.feedReward(0)
            self.p2.feedReward(1)
        else:
            self.p1.feedReward(0.3)
            self.p2.feedReward(0.3)

    def reset(self):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.boardHash = None
        self.isEnd = False
        self.playerSymbol = 1

    def play(self, rounds=100):
        for i in range(rounds):
            while not self.isEnd:
                # Player 1
                positions = self.availablePositions()
                action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
                self.updateState(action)
                self.p1.addState(self.getHash())

                win = self.winner()
                if win is not None:
                    self.giveReward()
                    self.p1.reset()
                    self.p2.reset()
                    self.reset()
                    break

                # Player 2
                positions = self.availablePositions()
                action = self.p2.chooseAction(positions, self.board, self.playerSymbol)
                self.updateState(action)
                self.p2.addState(self.getHash())

                win = self.winner()
                if win is not None:
                    self.giveReward()
                    self.p1.reset()
                    self.p2.reset()
                    self.reset()
                    break

    # üî•üî•üî• ONLY THIS FUNCTION IS UPDATED (3 moves limit)
    def play2(self):
        move_count = 0

        while not self.isEnd and move_count < 3:
            # Computer move
            positions = self.availablePositions()
            action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
            self.updateState(action)
            print("\nComputer move:")
            self.showBoard()
            move_count += 1

            if self.winner() is not None or move_count >= 3:
                break

            # Human move
            positions = self.availablePositions()
            action = self.p2.chooseAction(positions)
            self.updateState(action)
            print("\nYour move:")
            self.showBoard()
            move_count += 1

            if self.winner() is not None:
                break

        print("\nüëâ Game stopped after 3 moves.")
        self.reset()

    def showBoard(self):
        for i in range(BOARD_ROWS):
            print("+---+---+---+")
            row_out = "| "
            for j in range(BOARD_COLS):
                if self.board[i, j] == 1:
                    tok = 'X'
                elif self.board[i, j] == -1:
                    tok = 'O'
                else:
                    tok = ' '
                row_out += tok + " | "
            print(row_out)
        print("+---+---+---+")


class Player:
    def __init__(self, name, exp_rate=0.3):
        self.name = name
        self.states = []
        self.lr = 0.2
        self.exp_rate = exp_rate
        self.decay_gamma = 0.9
        self.states_value = {}

    def getHash(self, board):
        return str(board.reshape(BOARD_COLS * BOARD_ROWS))

    def chooseAction(self, positions, board, symbol):
        if np.random.uniform(0, 1) <= self.exp_rate:
            idx = np.random.choice(len(positions))
            return positions[idx]

        value_max = -999
        for p in positions:
            next_board = board.copy()
            next_board[p] = symbol
            next_hash = self.getHash(next_board)
            value = self.states_value.get(next_hash, 0)

            if value >= value_max:
                value_max = value
                action = p
        return action

    def addState(self, state):
        self.states.append(state)

    def feedReward(self, reward):
        for st in reversed(self.states):
            if st not in self.states_value:
                self.states_value[st] = 0
            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])
            reward = self.states_value[st]

    def reset(self):
        self.states = []

    def savePolicy(self):
        with open('policy_' + self.name, 'wb') as fw:
            pickle.dump(self.states_value, fw)

    def loadPolicy(self, file):
        with open(file, 'rb') as fr:
            self.states_value = pickle.load(fr)


class HumanPlayer:
    def __init__(self, name):
        self.name = name

    def chooseAction(self, positions):
        while True:
            print("Your turn! Enter row & column (0‚Äì2)")
            try:
                row = int(input("Row: "))
                col = int(input("Col: "))
                if (row, col) in positions:
                    return (row, col)
                print("Invalid move! Try again.")
            except:
                print("Enter numbers only!")


if __name__ == "__main__":
    # Train AI
    p1 = Player("p1")
    p2 = Player("p2")

    st = State(p1, p2)
    print("Training...")
    st.play(50000)

    p1.savePolicy()

    # Play with human
    p1 = Player("computer", exp_rate=0)
    p1.loadPolicy("policy_p1")

    p2 = HumanPlayer("You")

    st = State(p1, p2)

    c = 'y'
    while c == 'y':
        st.play2()
        c = input("Play again? (y/n): ").lower()

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

class AlexNetReduced(Sequential):
    def __init__(self, input_shape, num_classes):
        super().__init__()

        # 1st Convolution Block
        self.add(Conv2D(64, (11, 11), strides=4, activation='relu', input_shape=input_shape))
        self.add(MaxPooling2D((3, 3), strides=2))

        # 2nd Convolution Block
        self.add(Conv2D(192, (5, 5), padding='same', activation='relu'))
        self.add(MaxPooling2D((3, 3), strides=2))

        # ‚≠ê Reduced to ONE 3√ó3 Conv layer (instead of 3 layers)
        self.add(Conv2D(256, (3, 3), padding='same', activation='relu'))

        self.add(MaxPooling2D((3, 3), strides=2))

        # Flatten
        self.add(Flatten())

        # ‚≠ê Reduced Fully Connected Layers (from 4096 ‚Üí 1024 ‚Üí output)
        self.add(Dense(1024, activation='relu'))
        self.add(Dropout(0.5))

        # Output Layer
        self.add(Dense(num_classes, activation='softmax'))

# Example usage:
input_shape = (224, 224, 3)
num_classes = 1000
model = AlexNetReduced(input_shape, num_classes)
model.summary()

# -*- coding: utf-8 -*-
"""
Final Auto-Running Code (No Upload Needed)
Reduced-Parameter LSTM for Airline Passengers Forecasting
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import math

# -----------------------------------------
# Load dataset directly from URL
# -----------------------------------------
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"
data = pd.read_csv(url, skipfooter=5, engine='python')
print("Dataset Loaded Successfully")
print(data.head())

# -----------------------------------------
# DATA PREPARATION
# -----------------------------------------
dataset = data.iloc[:, 1].values.astype("float32").reshape(-1, 1)

scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)

train_size = int(len(dataset) * 0.75)
train, test = dataset[:train_size], dataset[train_size:]

# Sequence generator
def create_dataset(data, time_step=10):
    X, Y = [], []
    for i in range(len(data) - time_step - 1):
        X.append(data[i:(i + time_step), 0])
        Y.append(data[i + time_step, 0])
    return np.array(X), np.array(Y)

time_step = 10
trainX, trainY = create_dataset(train, time_step)
testX, testY = create_dataset(test, time_step)

trainX = trainX.reshape(trainX.shape[0], 1, trainX.shape[1])
testX = testX.reshape(testX.shape[0], 1, testX.shape[1])

# -----------------------------------------
# REDUCED-PARAMETER MODEL
# -----------------------------------------
model = Sequential()
model.add(LSTM(4, input_shape=(1, time_step)))
model.add(Dropout(0.2))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse')
model.summary()

model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=1)

# -----------------------------------------
# PREDICT
# -----------------------------------------
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)

trainPredict = scaler.inverse_transform(trainPredict)
testPredict = scaler.inverse_transform(testPredict)
trainY_inv = scaler.inverse_transform(trainY.reshape(-1, 1))
testY_inv = scaler.inverse_transform(testY.reshape(-1, 1))

# RMSE
trainScore = math.sqrt(mean_squared_error(trainY_inv, trainPredict))
testScore = math.sqrt(mean_squared_error(testY_inv, testPredict))

print("Train RMSE:", trainScore)
print("Test RMSE:", testScore)

# -----------------------------------------
# PLOT RESULTS
# -----------------------------------------
trainPlot = np.empty_like(dataset)
trainPlot[:] = np.nan
trainPlot[time_step:len(trainPredict)+time_step] = trainPredict

testPlot = np.empty_like(dataset)
testPlot[:] = np.nan
testPlot[len(trainPredict)+(time_step*2)+1:len(dataset)-1] = testPredict

plt.figure(figsize=(10,5))
plt.plot(scaler.inverse_transform(dataset), label='Real Data')
plt.plot(trainPlot, label='Train Prediction')
plt.plot(testPlot, label='Test Prediction')
plt.title("International Airline Passenger Forecasting (Reduced Model)")
plt.xlabel("Time")
plt.ylabel("Passengers")
plt.legend()
plt.show()